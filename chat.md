The evaluation-* files generated by the scripts appear to follow a naming convention that indicates the type of workload or evaluation being performed. Based on the workspace structure and the provided excerpts, the following files are relevant:

reverie-evaluation-sigcomm2023.cc: This file is the main simulation script for evaluating the Reverie system. It generates output files such as:

fcts.txt: Contains flow completion times (FCTs) and related metrics.
tor.txt: Contains statistics for ToR (Top-of-Rack) switches.
pfc.txt: Contains PFC (Priority Flow Control) events.
reverie-evaluation-workload-lakewood.sh and reverie-evaluation-workload-loveland.sh: These scripts likely execute specific workload evaluations. The file names suggest:

lakewood and loveland might refer to different workload configurations or datasets being evaluated.
File Name Format
The naming format evaluation-* indicates:

evaluation: Denotes that the file is related to performance evaluation or benchmarking.
sigcomm2023: Refers to a specific conference or publication (e.g., SIGCOMM 2023), indicating the context or purpose of the evaluation.
workload-lakewood / workload-loveland: Suggests different workload scenarios or configurations being tested.
In summary, the file name format provides context about the type of evaluation, the workload, and possibly the target audience or event for which the evaluation is conducted.

akash-mani
@workspace i see for eg FCTFILE=$DUMP_DIR/evaluation-$alg-$RDMACC-$TCPCC-$rdmaload-$tcpload-$rdmaburst-$tcpburst-$egresslossyFrac-$gamma.fct

what are these parameter numbers? check dump_sigcomm for examples of these files

The parameter numbers in the file name format (e.g., $alg, $RDMACC, $TCPCC, $rdmaload, $tcpload, $rdmaburst, $tcpburst, $egresslossyFrac, $gamma) represent specific configuration values used during the simulation. These parameters are dynamically substituted with actual values during the execution of the scripts, and the resulting files are stored in the dump_sigcomm/ directory.

Here’s what each parameter likely represents:

$alg: The algorithm being evaluated (e.g., DT, ABM, REVERIE).
$RDMACC: The RDMA congestion control algorithm (e.g., DCQCNCC, INTCC).
$TCPCC: The TCP congestion control algorithm (e.g., CUBIC, DCTCP).
$rdmaload: The RDMA load percentage (e.g., 0.2, 0.4, 0.6, 0.8).
$tcpload: The TCP load percentage (e.g., 0.2, 0.4, 0.6, 0.8).
$rdmaburst: The RDMA burst size (e.g., 500000, 1000000, 1500000).
$tcpburst: The TCP burst size (e.g., 500000, 1000000, 1500000).
$egresslossyFrac: The fraction of egress traffic that is lossy (e.g., 0.8).
$gamma: A parameter related to the workload or algorithm configuration (e.g., 0.999).
Examples in dump_sigcomm/
The dump_sigcomm/ directory contains files generated during the simulation, with names following the format:

For example:

evaluation-111-1-2-0.4-0.6-1000000-1500000-0.8-0.999.fct
evaluation-111-1-2-0.4-0.6-1000000-1500000-0.8-0.999.tor
evaluation-111-1-2-0.4-0.6-1000000-1500000-0.8-0.999.pfc
File Extensions
.fct: Contains flow completion time (FCT) data.
.tor: Contains ToR (Top-of-Rack) switch statistics, such as buffer occupancy.
.pfc: Contains Priority Flow Control (PFC) pause frame statistics.
.out: Likely contains raw simulation output or logs.
These files are used for post-simulation analysis, such as generating plots in sigcomm-plots.py. For example:

The .fct file is parsed to calculate metrics like average FCT, 95th percentile FCT, etc.
The .tor file is used to analyze buffer occupancy (lossless and lossy).
The .pfc file is used to count the number of PFC pause frames.
If you want to inspect specific examples, you can open files 


Here’s an explanation of the end-to-end flow in the provided code excerpt, starting from the input parameters:

1. Input Parameters
The key input parameters for the simulation are:

fromLeafId: The ID of the leaf switch initiating the RDMA traffic.
requestRate: The rate at which requests are generated (likely in requests per second).
requestSize: The size of each request in bytes.
cdfTable: A pointer to a structure that likely defines the distribution of flow sizes.
flowCount: A reference to a counter tracking the total number of flows generated.
SERVER_COUNT: The number of servers connected to each leaf switch.
LEAF_COUNT: The total number of leaf switches in the topology.
START_TIME: The simulation start time.
END_TIME: The simulation end time.
FLOW_LAUNCH_END_TIME: The time after which no new flows are launched.
2. Flow Generation Logic
The flow generation happens in the incast_rdma function:

a. Loop Over Servers
The function iterates over all servers connected to the fromLeafId leaf switch (SERVER_COUNT servers).
For each server, it calculates the start time of the flow using START_TIME and a random interval generated by poission_gen_interval(requestRate).
b. Flow Scheduling
A while loop ensures that flows are generated as long as:
The calculated startTime is within the range [START_TIME, FLOW_LAUNCH_END_TIME].
c. Destination Selection
The destination leaf switch (leaftarget) is determined as the next leaf switch in the topology. If the current leaf is the last one, it wraps around to the first leaf.
The destination server index (destServerIndex) is set to the same server index as the source server.
d. Flow Size Calculation
The total request size is divided by the fan-out (fan) to determine the size of each flow.
e. Port Management
Source (sport) and destination (dport) ports are assigned for the flow. If no port is assigned yet, a random port in the range [10000, 11000] is selected.
f. Flow Creation
An RDMA flow is created using the RdmaClientHelper class, which sets up the client-side application for the flow.
The flow is installed on the source server (n.Get(fromServerIndex)), and its start time is scheduled using appCon.Start(Seconds(startTime)).
g. Flow Count Update
The flowCount variable is incremented to track the total number of flows generated.
3. Traffic Pattern
The incast_rdma function generates an incast traffic pattern, where multiple servers send data to a single destination server. This is evident from the loop over the fan variable, which creates multiple flows from different servers to the same destination.

4. Simulation Timing
The simulation starts at START_TIME.
Flows are generated until FLOW_LAUNCH_END_TIME.
The simulation likely ends at END_TIME, though this is not explicitly shown in the provided code.
5. High-Level Flow
Input Parameters: The simulation is configured with parameters like the number of servers, leaf switches, request rate, and timing.
Flow Generation: The incast_rdma function generates flows based on the input parameters, scheduling them to start at specific times.
Traffic Pattern: The generated flows follow an incast pattern, with multiple servers sending data to a single destination.
Simulation Execution: The flows are executed during the simulation, with their start times and durations controlled by the input parameters.
If you need further clarification or details about specific parts of the code, let me know!




# Explanation of NS-3 Simulation Output Fields

Based on the Reverie paper, I can explain the three output files from your NS-3 datacenter network simulation:

## PFC File (.pfc)
This tracks Priority Flow Control events in the network:

- **Time**: Timestamp when the PFC event occurred (in seconds)
- **NodeId**: Identifier for the network device (266, 170, etc.)
- **NodeType**: Device type (0 = host, 1 = switch)
- **IfIndex**: Interface/port number on the device
- **type**: PFC event type (1 = PAUSE frame sent, 2 = RESUME frame sent)

In your data, switches (NodeType 1) are sending RESUME frames (type 2) to multiple ports, while hosts (NodeType 0) are sending PAUSE frames (type 1) at timestamp 1.02748s.

## Flow Completion Time File (.fct)
This tracks the performance of individual network flows:

- **timestamp**: When the flow completed (in seconds)
- **flowsize**: Size of the flow in bytes (all 125000 bytes in your data)
- **fctus**: Actual flow completion time in microseconds
- **basefctus**: Theoretical minimum flow completion time (59200 μs)
- **slowdown**: Ratio of actual to theoretical completion time (key performance metric)
- **baserttus**: Base round-trip time in microseconds (17280 μs)
- **priority**: QoS priority level of the flow (all priority 3)
- **incastflow**: Whether this is part of an incast pattern (1 = yes)

Your data shows several completed incast flows with varying slowdowns (3.1-7.4x), indicating congestion.

## ToR Switch Statistics File (.tor)
This tracks buffer utilization in switches:

- **switch**: Switch identifier (0-15, covering all ToR switches)
- **totalused**: Total buffer space used
- **egressOccupancyLossless**: Buffer occupied by lossless (RDMA) traffic at egress
- **egressOccupancyLossy**: Buffer occupied by lossy (TCP) traffic at egress
- **ingressPoolOccupancy**: Occupancy of the ingress buffer pool
- **headroomOccupancy**: Occupancy of the PFC headroom pool (reserved for lossless traffic after PFC PAUSE)
- **sharedPoolOccupancy**: Occupancy of the shared buffer pool (for Reverie model)
- **time**: Timestamp (in seconds)

Your data shows switches with no buffer utilization at the beginning of the simulation (time 1.0s).

These outputs are critical for evaluating how different buffer management schemes (DT, ABM, or Reverie) handle the complex interactions between RDMA and TCP traffic in datacenter networks.

